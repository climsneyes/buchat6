import os
import pickle
import hashlib
import time
from typing import List, Dict, Any
import PyPDF2
import chromadb
from chromadb.config import Settings
import openai
from config import OPENAI_API_KEY

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = openai.OpenAI(api_key=OPENAI_API_KEY)

class OpenAIEmbeddingFunction:
    """OpenAI ì„ë² ë”© í•¨ìˆ˜"""
    def __init__(self, api_key: str, model: str = "text-embedding-3-small"):
        self.api_key = api_key
        self.model = model
        self.client = openai.OpenAI(api_key=api_key)
    
    def __call__(self, input: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            response = self.client.embeddings.create(
                input=input,
                model=self.model
            )
            return [embedding.embedding for embedding in response.data]
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def name(self):
        return "openai"

class ChromaDBWrapper:
    def __init__(self, db_name: str, persist_directory: str = "./chroma_db"):
        """ChromaDB ë˜í¼ í´ë˜ìŠ¤"""
        self.db_name = db_name
        self.persist_directory = persist_directory
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # OpenAI ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©
        embedding_function = OpenAIEmbeddingFunction(OPENAI_API_KEY)
        
        self.collection = self.client.get_or_create_collection(
            name=db_name,
            embedding_function=embedding_function,
            metadata={"hnsw:space": "cosine"}
        )
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """ë¬¸ì„œë“¤ì„ ChromaDBì— ì¶”ê°€"""
        ids = []
        texts = []
        metadatas = []
        
        for i, doc in enumerate(documents):
            doc_id = f"doc_{i}_{hashlib.md5(doc['page_content'].encode()).hexdigest()[:8]}"
            ids.append(doc_id)
            texts.append(doc['page_content'])
            metadatas.append(doc['metadata'])
        
        print(f"ğŸ“ ChromaDBì— {len(documents)}ê°œ ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ëŠ” ì¤‘...")
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        batch_size = 50
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            
            try:
                self.collection.add(
                    documents=batch_texts,
                    metadatas=batch_metadatas,
                    ids=batch_ids
                )
                print(f"   âœ… ë°°ì¹˜ {i//batch_size + 1} ì™„ë£Œ ({len(batch_texts)}ê°œ ë¬¸ì„œ)")
            except Exception as e:
                print(f"   âŒ ë°°ì¹˜ {i//batch_size + 1} ì˜¤ë¥˜: {e}")
        
        print(f"âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œë¥¼ ChromaDB '{self.db_name}'ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
    
    def similarity_search(self, query: str, k: int = 3):
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        results = self.collection.query(
            query_texts=[query],
            n_results=k
        )
        return results

def chunk_pdf_to_text_chunks(pdf_path: str, chunk_size: int = 1000, chunk_overlap: int = 100) -> List[Dict[str, Any]]:
    """PDFë¥¼ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            
            for page_num, page in enumerate(pdf_reader.pages):
                text = page.extract_text()
                if not text.strip():
                    continue
                
                # í˜ì´ì§€ë³„ë¡œ ì²­í¬ ë¶„í• 
                words = text.split()
                for i in range(0, len(words), chunk_size - chunk_overlap):
                    chunk_words = words[i:i + chunk_size]
                    chunk_text = ' '.join(chunk_words)
                    
                    if chunk_text.strip():
                        chunks.append({
                            'page_content': chunk_text,
                            'metadata': {
                                'source': pdf_path,
                                'page': page_num + 1,
                                'chunk_id': len(chunks)
                            }
                        })
    
    except Exception as e:
        print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜ ({pdf_path}): {e}")
        return []
    
    return chunks

def create_foreign_worker_db():
    """ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìˆ˜ì²© PDFë“¤ì„ ì„ë² ë”©í•´ì„œ ChromaDBë¡œ ì €ì¥"""
    
    # PDF íŒŒì¼ ëª©ë¡
    pdf_files = [
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìˆ˜ì²©_(êµ­ë¬¸ìš©).pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìš°ì¦ˆë°±.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ë„¤íŒ”ì–´.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ë™í‹°ëª¨ë¥´.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ë¼ì˜¤ìŠ¤.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ëª½ê³¨ì–´.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ë¯¸ì–€ë§ˆ.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ë°©ê¸€ë¼ë°ì‹œ.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ë² íŠ¸ë‚¨.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ìŠ¤ë¦¬ë‘ì¹´.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ì¸ë„ë„¤ì‹œì•„.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_ìº„ë³´ë””ì•„.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_í‚¤ë¥´ê¸°ìŠ¤.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_íƒœêµ­ì–´.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_íŒŒí‚¤ìŠ¤íƒ„.pdf",
        "ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìš©ìˆ˜ì²©_ìµœì¢…_í•„ë¦¬í•€.pdf"
    ]
    
    # ChromaDB ìƒì„± (ê¸°ì¡´ DBì™€ ê²¹ì¹˜ì§€ ì•ŠëŠ” ì´ë¦„)
    db_name = "foreign_worker_rights_guide_openai"
    chroma_db = ChromaDBWrapper(db_name)
    
    all_chunks = []
    
    print("ğŸ” PDF íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
    
    for pdf_file in pdf_files:
        if not os.path.exists(pdf_file):
            print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_file}")
            continue
        
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {pdf_file}")
        chunks = chunk_pdf_to_text_chunks(pdf_file)
        all_chunks.extend(chunks)
        print(f"   â†’ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    if not all_chunks:
        print("âŒ ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    print(f"\nğŸ“Š ì´ {len(all_chunks)}ê°œ ì²­í¬ë¥¼ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤...")
    
    # ChromaDBì— ë¬¸ì„œ ì¶”ê°€
    chroma_db.add_documents(all_chunks)
    
    # DB ì •ë³´ ì €ì¥
    db_info = {
        'db_name': db_name,
        'total_chunks': len(all_chunks),
        'pdf_files': pdf_files,
        'created_at': time.time()
    }
    
    with open(f"{db_name}_info.pkl", "wb") as f:
        pickle.dump(db_info, f)
    
    print(f"âœ… ì™¸êµ­ì¸ë…¸ë™ìê¶Œë¦¬êµ¬ì œì•ˆë‚´ìˆ˜ì²© ChromaDB ìƒì„± ì™„ë£Œ!")
    print(f"   - DB ì´ë¦„: {db_name}")
    print(f"   - ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}")
    print(f"   - ì €ì¥ ìœ„ì¹˜: ./chroma_db/{db_name}")
    print(f"   - ì •ë³´ íŒŒì¼: {db_name}_info.pkl")
    
    return chroma_db

if __name__ == "__main__":
    create_foreign_worker_db() 